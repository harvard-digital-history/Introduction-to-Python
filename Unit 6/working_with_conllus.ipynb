{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "868f813e",
   "metadata": {},
   "source": [
    "# Working with CoNLL-Us\n",
    "\n",
    "CoNLL-Us and CoNLL-Xs are two highly similar types of files used for storing annotated linguistic data. They are explicitly designed to express their data in the [Universal Dependencies](https://universaldependencies.org/), as will be seen below.\n",
    "\n",
    "Below will explore three ways of working with CoNLL files, using a sample sentence (with lots of non-punctuation periods!). We will work through the structure of CoNLL files as we explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5144208",
   "metadata": {},
   "source": [
    "## The conllu package\n",
    "\n",
    "One of the most efficient ways to work with CoNLL files in Python is the [`conllu` package](https://pypi.org/project/conllu/). As always, you need to install this with pip first, using `pip install conllu` in the anaconda prompt. Then, import it with the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d6feff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3473408",
   "metadata": {},
   "source": [
    "For our purposes we will use a series of annotated sentences made with Stanza. You may uncomment the line below to work with a set generated by Trankit, but this has less metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26bd0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_file = 'test_stanza.conllu'\n",
    "#annotated_file = 'test_eng_trankit.conllu'\n",
    "\n",
    "with open(annotated_file, encoding='utf-8') as f:\n",
    "    sentences = parse(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f1b49",
   "metadata": {},
   "source": [
    "`sentences` is a list of each sentence (separated by two line breaks instead of one) in the document. Let's take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62fbb5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 sentences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TokenList<\", Hello, Mr., Black, ,, \", J., J., said, ., metadata={text: \"\"Hello Mr. Black,\" J. J. said.\", sent_id: \"0\"}>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Found {len(sentences)} sentences.')\n",
    "sample_sen = sentences[0]\n",
    "sample_sen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b92f57",
   "metadata": {},
   "source": [
    "That looks ugly! Let's take a closer look at what's going on.\n",
    "\n",
    "Every sentence in the `conllu` library is rendered as a `TokenList` that includes `metadata`, indicated in the file with lines beginning with `#` before the tokens of the sentence. `metadata` behaves like a dict, and in this case (thanks to Stanza) includes `text`, which is the unedited, untokenized sentence. Let's see what we're dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a5fbee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hello Mr. Black,\" J. J. said.\n"
     ]
    }
   ],
   "source": [
    "print(sample_sen.metadata.get('text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0ef66",
   "metadata": {},
   "source": [
    "The main elements of the sentence are split into _tokens_ and accessed as entries in the `TokenList`, which behaves as an iterator. Note that printing and outputting the tokens lead to different results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35131e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Hello\n",
      "Mr.\n",
      "Black\n",
      ",\n",
      "\"\n",
      "J.\n",
      "J.\n",
      "said\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for t in sample_sen:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290b9a0",
   "metadata": {},
   "source": [
    "but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d53ebef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 9,\n",
       " 'form': 'said',\n",
       " 'lemma': 'say',\n",
       " 'upos': 'VERB',\n",
       " 'xpos': 'VBD',\n",
       " 'feats': {'Mood': 'Ind',\n",
       "  'Number': 'Sing',\n",
       "  'Person': '3',\n",
       "  'Tense': 'Past',\n",
       "  'VerbForm': 'Fin'},\n",
       " 'head': 0,\n",
       " 'deprel': 'root',\n",
       " 'deps': None,\n",
       " 'misc': {'start_char': '25', 'end_char': '29'}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "said = sample_sen[-2]\n",
    "said"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176a24e",
   "metadata": {},
   "source": [
    "Note that tokens are really encoded as dictionaries. To see what's fully going on, let's look at each of these elements. They correspond to the standard columns in a CoNLL file:\n",
    "\n",
    "* `id`: the place of the word in the sentence\n",
    "* `form`: the exact form the token takes on in the sentence\n",
    "* `lemma`: the dictionary or generalized form of the word\n",
    "* `upos`: the POS in accordance with Universal Dependencies POS tags\n",
    "* `xpos` (optional): an alternative POS schema. I don't know what this corresponds to in the particular case of Stanza's English pipeline.\n",
    "* `feats`: Encoded as a dictionary. Specifies the grammatical or morphological features the word takes on, e.g. `Tense: Past` means the verb is a past-tense form.\n",
    "* `head`: the index on which this word is deemed to depend syntactically. 0 is reserved for the `root` position, which indicates this is the main element of the sentence.\n",
    "* `deprel`: how this word relates to the head\n",
    "* `deps`: Advanced, an alternative way of representing dependencies.\n",
    "* `misc`: Additional information. Stanza helpfully specifies the [start_char, end_char) of the token in the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6707b54c",
   "metadata": {},
   "source": [
    "You can iterate through the `TokenList` to modify or identify words. For instance, if we want to find all verbs, we might do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d902f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 9, 'form': 'said', 'lemma': 'say', 'upos': 'VERB', 'xpos': 'VBD', 'feats': {'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Past', 'VerbForm': 'Fin'}, 'head': 0, 'deprel': 'root', 'deps': None, 'misc': {'start_char': '25', 'end_char': '29'}}]\n"
     ]
    }
   ],
   "source": [
    "verbs = []\n",
    "for t in sample_sen:\n",
    "    if t.get('upos') == 'VERB':\n",
    "        verbs.append(t)\n",
    "\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df09402b",
   "metadata": {},
   "source": [
    "If we wanted to, say, delete all periods that do _not_ serve a punctuation function in a sentence, we could do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4531a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Hello Mr Black,\" J J said.\n"
     ]
    }
   ],
   "source": [
    "def replace_nonpunct_periods(sen):\n",
    "    punct = ['\"', \"'\", '.',',']\n",
    "    for t in sen:\n",
    "        if t.get('upos') != 'PUNCT':\n",
    "            t['form'] = t['form'].replace('.', '')\n",
    "            t['lemma'] = t['lemma'].replace('.', '')\n",
    "    \n",
    "    sen.metadata['text'] = \" \".join([t.get('form') for t in sen]) #Not perfect but it'll do\n",
    "    for p in punct:\n",
    "        sen.metadata['text'] = sen.metadata['text'].replace(f' {p}', p)\n",
    "\n",
    "replace_nonpunct_periods(sample_sen)\n",
    "print(sample_sen.metadata.get('text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c3f51",
   "metadata": {},
   "source": [
    "What if we want to return this to a .conllu? We can use the `serialize()` to turn it back into a CoNLL-u and then write to a file!\n",
    "\n",
    "**Hint**: Remember to serialize *individual sentences*, not the whole list of sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8084c138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# text = \" Hello Mr Black,\" J J said.\n",
      "# sent_id = 0\n",
      "1\t\"\t\"\tPUNCT\t``\t_\t2\tpunct\t_\tstart_char=0|end_char=1\n",
      "2\tHello\thello\tINTJ\tUH\t_\t9\tccomp\t_\tstart_char=1|end_char=6\n",
      "3\tMr\tMr\tPROPN\tNNP\tNumber=Sing\t2\tvocative\t_\tstart_char=7|end_char=10\n",
      "4\tBlack\tBlack\tPROPN\tNNP\tNumber=Sing\t3\tflat\t_\tstart_char=11|end_char=16\n",
      "5\t,\t,\tPUNCT\t,\t_\t9\tpunct\t_\tstart_char=16|end_char=17\n",
      "6\t\"\t\"\tPUNCT\t''\t_\t9\tpunct\t_\tstart_char=17|end_char=18\n",
      "7\tJ\tJ\tPROPN\tNNP\tNumber=Sing\t9\tnsubj\t_\tstart_char=19|end_char=21\n",
      "8\tJ\tJ\tPROPN\tNNP\tNumber=Sing\t7\tflat\t_\tstart_char=22|end_char=24\n",
      "9\tsaid\tsay\tVERB\tVBD\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\t0\troot\t_\tstart_char=25|end_char=29\n",
      "10\t.\t.\tPUNCT\t.\t_\t9\tpunct\t_\tstart_char=29|end_char=30\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_conllu = sample_sen.serialize()\n",
    "print(new_conllu)\n",
    "\n",
    "with open('modified_conllu.conllu', 'w', encoding='utf-8') as f:\n",
    "    f.write(new_conllu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b96901",
   "metadata": {},
   "source": [
    "## Visualizing CoNLL-Us: Palmyra:\n",
    "\n",
    "You can also modify and visualize CoNLL files visually. This is especially useful for manually correcting the syntax so you can train a model or quantitatively assess later.\n",
    "\n",
    "I recomend using [Palmyra](https://camel-lab.github.io/palmyra/viewtree.html) by CamelTools. For this, choose `ud.config` and then upload the file. We'll go over this visually in class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43a863",
   "metadata": {},
   "source": [
    "## CoNLL-Us as .TSVs:\n",
    "\n",
    "CoNLL-Us are, at their core, tab-separated spreadsheets. It is therefore possible to work with CoNLL-Us using the csv library. This is better for *writing* CoNLL files than *reading* them, and we will not cover this in depth, but we will present ways to load them in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e2ac0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf63ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# text = \"Hello Mr. Black,\" J. J. said.']\n",
      "['# sent_id = 0']\n",
      "['1', '\\t', 'PUNCT', '``', '_', '2', 'punct', '_', 'start_char=0|end_char=1']\n",
      "['2', 'Hello', 'hello', 'INTJ', 'UH', '_', '9', 'ccomp', '_', 'start_char=1|end_char=6']\n",
      "['3', 'Mr.', 'Mr.', 'PROPN', 'NNP', 'Number=Sing', '2', 'vocative', '_', 'start_char=7|end_char=10']\n",
      "['4', 'Black', 'Black', 'PROPN', 'NNP', 'Number=Sing', '3', 'flat', '_', 'start_char=11|end_char=16']\n",
      "['5', ',', ',', 'PUNCT', ',', '_', '9', 'punct', '_', 'start_char=16|end_char=17']\n",
      "['6', '\\t', 'PUNCT', \"''\", '_', '9', 'punct', '_', 'start_char=17|end_char=18']\n",
      "['7', 'J.', 'J.', 'PROPN', 'NNP', 'Number=Sing', '9', 'nsubj', '_', 'start_char=19|end_char=21']\n",
      "['8', 'J.', 'J.', 'PROPN', 'NNP', 'Number=Sing', '7', 'flat', '_', 'start_char=22|end_char=24']\n",
      "['9', 'said', 'say', 'VERB', 'VBD', 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin', '0', 'root', '_', 'start_char=25|end_char=29']\n",
      "['10', '.', '.', 'PUNCT', '.', '_', '9', 'punct', '_', 'start_char=29|end_char=30']\n",
      "[]\n",
      "['# text = \"Oh, hello J.J.,\" mrs. smith added.']\n",
      "['# sent_id = 1']\n",
      "['1', '\\t', 'PUNCT', '``', '_', '4', 'punct', '_', 'start_char=31|end_char=32']\n",
      "['2', 'Oh', 'oh', 'INTJ', 'UH', '_', '4', 'discourse', '_', 'start_char=32|end_char=34']\n",
      "['3', ',', ',', 'PUNCT', ',', '_', '4', 'punct', '_', 'start_char=34|end_char=35']\n",
      "['4', 'hello', 'hello', 'INTJ', 'UH', '_', '10', 'ccomp', '_', 'start_char=36|end_char=41']\n",
      "['5', 'J.J.', 'J.J.', 'PROPN', 'NNP', 'Number=Sing', '4', 'vocative', '_', 'start_char=42|end_char=46']\n",
      "['6', ',', ',', 'PUNCT', ',', '_', '4', 'punct', '_', 'start_char=46|end_char=47']\n",
      "['7', '\\t', 'PUNCT', \"''\", '_', '4', 'punct', '_', 'start_char=47|end_char=48']\n",
      "['8', 'mrs.', 'mrs.', 'PROPN', 'NNP', 'Number=Sing', '10', 'nsubj', '_', 'start_char=49|end_char=53']\n",
      "['9', 'smith', 'smith', 'PROPN', 'NNP', 'Number=Sing', '8', 'flat', '_', 'start_char=54|end_char=59']\n",
      "['10', 'added', 'add', 'VERB', 'VBD', 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin', '0', 'root', '_', 'start_char=60|end_char=65']\n",
      "['11', '.', '.', 'PUNCT', '.', '_', '10', 'punct', '_', 'start_char=65|end_char=66']\n",
      "[]\n",
      "['# text = \"Q. E. D.,\" M. m. smith replied sadly.']\n",
      "['# sent_id = 2']\n",
      "['1', '\\t', 'PUNCT', '``', '_', '2', 'punct', '_', 'start_char=67|end_char=68']\n",
      "['2', 'Q.', 'Q.', 'PROPN', 'NNP', 'Number=Sing', '10', 'ccomp', '_', 'start_char=68|end_char=70']\n",
      "['3', 'E.', 'E.', 'PROPN', 'NNP', 'Number=Sing', '2', 'flat', '_', 'start_char=71|end_char=73']\n",
      "['4', 'D.', 'D.', 'PROPN', 'NNP', 'Number=Sing', '2', 'flat', '_', 'start_char=74|end_char=76']\n",
      "['5', ',', ',', 'PUNCT', ',', '_', '10', 'punct', '_', 'start_char=76|end_char=77']\n",
      "['6', '\\t', 'PUNCT', \"''\", '_', '10', 'punct', '_', 'start_char=77|end_char=78']\n",
      "['7', 'M.', 'M.', 'PROPN', 'NNP', 'Number=Sing', '10', 'nsubj', '_', 'start_char=79|end_char=81']\n",
      "['8', 'm.', 'marry', 'PROPN', 'NNP', 'Number=Sing', '7', 'flat', '_', 'start_char=82|end_char=84']\n",
      "['9', 'smith', 'smith', 'PROPN', 'NNP', 'Number=Sing', '7', 'flat', '_', 'start_char=85|end_char=90']\n",
      "['10', 'replied', 'reply', 'VERB', 'VBD', 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin', '0', 'root', '_', 'start_char=91|end_char=98']\n",
      "['11', 'sadly', 'sadly', 'ADV', 'RB', '_', '10', 'advmod', '_', 'start_char=99|end_char=104']\n",
      "['12', '.', '.', 'PUNCT', '.', '_', '10', 'punct', '_', 'start_char=104|end_char=105']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "with open(annotated_file, encoding='utf8') as f:\n",
    "    tsv = list(csv.reader(f, delimiter='\\t'))\n",
    "\n",
    "for l in tsv:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b424f",
   "metadata": {},
   "source": [
    "Note that it breaks around the quotation marks! As an exercise, let's get a set of all UPOS in this corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33cf5e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADV', \"''\", 'VERB', '``', 'INTJ', 'PROPN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "upos_list = []\n",
    "\n",
    "for r in tsv:\n",
    "    if(len(r) > 4):\n",
    "        upos_list.append(r[3])\n",
    "\n",
    "print(list(set(upos_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2370c6",
   "metadata": {},
   "source": [
    "Well, it's better than nothing. Yeah don't use .csv to read, just to write."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
