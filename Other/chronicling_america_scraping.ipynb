{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Chronicling America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium/Chomedriver/Chrome: https://googlechromelabs.github.io/chrome-for-testing/\n",
    "\n",
    "\n",
    "We'll use the list view instead of the gallery view (so we can get 50 results\n",
    "at a time and avoid unnecessary complications with the javascript used\n",
    "for handling thumbnails). The url can be obtained by doing a manual search and then copying the resulting URL and split it into a base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://chroniclingamerica.loc.gov/search/pages/results/list'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a dictionary of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'date1': 1789,  # default: 1789\n",
    "    'rows': 50,  # default: 20\n",
    "    'searchType': 'basic',  # default: basic\n",
    "    'state': '',  # default: '' (empty)\n",
    "    'date2': 1963,  # default: 1963\n",
    "    'proxtext': '',\n",
    "    'y': 12,  # default: 12\n",
    "    'x': 13,  # default: 13\n",
    "    'dateFilterType': 'yearRange',  # default: yearRange\n",
    "    'page': 1,\n",
    "    'sort': 'relevance',  # default: relevance\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also assumed that you'll want to do multiple searches, so\n",
    "search_terms is a list of lists where each sublist is a collection\n",
    "of terms to search for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = [\n",
    "    ['thanksgiving', 'indian'],\n",
    "    # ['foo', 'bar']  # etc.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup the selenium driver **(adjust path as necessary)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service()\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=service, options=options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ancillary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a couple of functions to which we can pass raw data and get processed results when we loop over pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_page(page):\n",
    "    \"\"\"Take a page number and returns the appropiate URL based on base_url and the parameters dictionary.\"\"\"\n",
    "    url = f'{base_url}/?'  # we add /? to start the parameters string\n",
    "    parameters.update({'page': page})  # we update the dictionary with page\n",
    "    count = 0  # to keep track of which parameter we're adding\n",
    "    # iterate over the dictionary and append to the base URL\n",
    "    for key, value in parameters.items():\n",
    "        # if this is the first parameter we append it as-is\n",
    "        # otherwise we prepend &\n",
    "        url += f'{key}={value}' if count == 0 else f'&{key}={value}'\n",
    "        count += 1  # increment the count by 1  # noqa: SIM113\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_text(url):\n",
    "    \"\"\"Take the URL for a page and returns a dictionary containing data scraped from that page.\"\"\"\n",
    "    # the URLS we scraped have the format:\n",
    "    # https://chroniclingamerica.loc.gov/lccn/2017270500/1915-01-01/ed-1\n",
    "    # /seq-17/;words=[u'Indian',%20u'Thanksgiving']?date1=1789&rows [...]\n",
    "    # so to get the base url we split on ';' (right before words)\n",
    "    url_base, url_paras = url.split(';')\n",
    "    driver.get(f'{url_base}ocr/')  # the text page is just the base url + /ocr/\n",
    "    # we return a dictionary with the elements we want to capture\n",
    "    # I selected a few basic things, just add more as necessary\n",
    "    return {\n",
    "        'title': driver.find_elements_by_css_selector('div[id=head_nav] h1')[1].text,\n",
    "        'credit_line': driver.find_elements_by_css_selector('div[id=head_nav] h3')[0].text,\n",
    "        'text': driver.find_elements_by_css_selector('p')[0].text,\n",
    "        'purl': url_base,\n",
    "        'raw_text_url': f'{url_base}ocr.txt',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs once for each set of search_terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for search_terms_set in search_terms:\n",
    "    # lists for holding results links and content\n",
    "    newspaper_links = []\n",
    "    newspaper_text = []\n",
    "\n",
    "    # update parameters dictionary with search terms\n",
    "    parameters.update({\n",
    "        'proxtext': '+'.join(search_terms_set)\n",
    "    })\n",
    "\n",
    "    # LOOP 1: Collect URLs from each results page\n",
    "    # first we get the total number of result pages\n",
    "    driver.get(get_results_page(1))  # we go to the first page\n",
    "    # look at the pagination in the top-left corner, it is a bunch of <a> tags\n",
    "    # nested inside a <span> with class = pagination\n",
    "    # we get the text from the penultimate one in the list\n",
    "    # (the last one is the [next] button)\n",
    "    result_page_count = driver.find_elements(By.CSS_SELECTOR, 'span[class=pagination] a')[-2].text\n",
    "    # now we iterate over the results pages and capture all the links\n",
    "    # to individual results, 50 at a time\n",
    "    for page in range(1, int(result_page_count)):\n",
    "        driver.get(get_results_page(page))\n",
    "        links = [i.get_attribute('href') for i in driver.find_elements(By.CSS_SELECTOR, '.results_list>li>a')]\n",
    "        newspaper_links += links\n",
    "\n",
    "    # LOOP 2: Iterate through links and get content\n",
    "    for url in newspaper_links:\n",
    "        newspaper_text.append(scrape_text(url))  # for each link we call the scraping function\n",
    "\n",
    "    # SAVE THE RESULTS:\n",
    "    file_path = './'  # add a path as necessary\n",
    "    file_name = f'ca_scraping_{\"_\".join(search_terms_set)}'  # this makes each results set unique based on search terms\n",
    "    txt_folder = f'txt_{\"_\".join(search_terms_set)}'  # folder for individual text files\n",
    "\n",
    "    # save results to JSON -> compact format that saves all the data to a single file\n",
    "    with open(f'{file_path}{file_name}.json', 'w') as file:\n",
    "        file.write(json.dumps(newspaper_text))\n",
    "\n",
    "    # save results to TXT and CSV -> more flexible format, sp. for text analysis\n",
    "    # saves an index as a .csv file and the text as independent .txt files in a folder\n",
    "    csv_list = []  # list to hold CSV data\n",
    "\n",
    "    # try creating the folder for the txt files\n",
    "    try:\n",
    "        os.mkdir(f'{file_path}{txt_folder}')  # noqa: PTH102\n",
    "    # if it already exists, offer to overwrite the existing files\n",
    "    # and wait for confirmation just in case we don't want to do that!\n",
    "    except FileExistsError:\n",
    "        response = ''\n",
    "        while response not in ['y', 'n']:\n",
    "            response = input(f'The folder {txt_folder} already exists. Overwite contents [Y/N]?').lower()\n",
    "        if response == 'n':\n",
    "            sys.exit()  # if we don't want to overwrite the files, we exit the script\n",
    "\n",
    "    for index, result in enumerate(newspaper_text, start=1):\n",
    "        text = result.pop('text')  # remove the text from the result dictionary\n",
    "        result['filename'] = f'{index}.txt'  # add the name of the text file for reference\n",
    "        csv_list.append(result)  # add it to the list we'll turn into a csv\n",
    "\n",
    "        # write the text file to disk\n",
    "        with open(f'{file_path}{txt_folder}/{index}.txt', 'w') as file:\n",
    "            file.write(text)\n",
    "\n",
    "    # finally, we write the csv\n",
    "    with open(f'{file_path}{file_name}.csv', 'w') as file:\n",
    "        headers = csv_list[0].keys()  # we get the \"headers\" by extracting the keys to the first dictionary\n",
    "        dict_writer = csv.DictWriter(file, headers)  # we create a csv writers with the headers\n",
    "        dict_writer.writeheader()  # we write the headers first\n",
    "        dict_writer.writerows(csv_list)  # and then we write the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()  # lastly we clean after ourselves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
